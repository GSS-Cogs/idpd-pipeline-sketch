{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b9f174-941c-4e12-8511-cfb6711e49cc",
   "metadata": {},
   "source": [
    "# Pure Python Pipeline - Initial sketch\n",
    "\n",
    "The intention here is to sketch our the functions, classes and interactions required to run a purely python data ingestion pipeline.\n",
    "\n",
    "It's an important design consideration to try and enforce disintct decoupling and seperation of concern between the thing (the sequence of operations that inform/power a pipeline) and the runner (the thing running the pipeline) such that the runner can be freely changed as needed.\n",
    "\n",
    "## Concepts\n",
    "\n",
    "It's worth understanding a few high level concepts before reading the code in this notebook - as follows:\n",
    "\n",
    "\n",
    "### store\n",
    "\n",
    "All versions of this pipeline run against a directory of files or (in the case of buckets) an abstract representation of this.\n",
    "\n",
    "So our \"store\" is a class for interacting with a directory of files in such a way that the code doesn't care if its AWS bucket, local directory, google cloud storage, github etc under the hood - this means all interactions are standard regardless of the directory storage backend. That way  you can freely swap out that storage backend (for example: while developing or when running tests) and not alter the pipeline code.\n",
    "\n",
    "It _also_ means that since the store class is really just _references to files in a place_ it's _json serialisable_, something that is potentially of use when used with certain runners (airflow for example, can only pass variables between tasks if they can be json serialised).\n",
    "\n",
    "The abstract for this class (the place that enforces that all variation of store behave the same is) `BaseWritableSingleDirectoryStore` but the easier way to understand what this is is to look at `FakeS3DirectoryStore` in `./deps/store.s32fake.py` - all the methods should have sensible descriptions of what they do.\n",
    "\n",
    "I'd **strongly** recommend looking at that file first, the rest of this example will make far more sense then.\n",
    "\n",
    "### deps.other\n",
    "\n",
    "There other bits of reusable functionality we'll need but I'm not sure yet where they'd go so all associated functions are (for the sake of this example) just getting imported from here.\n",
    "\n",
    "As with the fake store they're just empty or have hard coded returns.\n",
    "\n",
    "Again, worth a read to understand the intention, I've tried to add decent comments.\n",
    "\n",
    "### notify\n",
    "\n",
    "We know that these pipelines need to notify various parties upon various events (in particular when something goes wrong).\n",
    "\n",
    "The \"notifier\" is intended to handle that in one place in a convenient fashion, i.e `notify.data_engineering(\"Something DEs need to know\")`.\n",
    "\n",
    "The idea is that _how_ it does that happens in this one central place, so if we want to change the notifying logic (such as to send a message to MS Teams rather than Slack) we only need to change the code in one place.\n",
    "\n",
    "### messages\n",
    "\n",
    "Given we're sending many messages it made sense to pull them out and assembled them via functions so we can manage them separate to the code.\n",
    "\n",
    "Doesn't matter much right now, but when you start thinking about formatting complex emails and the like, it'd be nice to have all that separated out so the pipeline code only deals with running the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2699f791-374e-4685-9dbe-982f5a769679",
   "metadata": {},
   "source": [
    "# Overarching Design\n",
    "\n",
    "This is intended to be pipelines in the form of **decoupled and simple python functions**.\n",
    "\n",
    "_Where_ we happend to run this code is not a consideration at this point and all efforts should be taken to avoid bringining logic from the runner (airflow, lambdas, whatever) into scope.\n",
    "\n",
    "The ideal is we build a thing that could literally just be the following functions be called via a script, we're then free to utilise that process on _any_ runer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38a7eef-8b94-456b-b3d4-0791ae83ff10",
   "metadata": {},
   "source": [
    "# Function 1 - Confirm Configuration\n",
    "\n",
    "I'm envisioing this as the very first step that's triggered after some amount of files appear in some directory like structure.\n",
    "\n",
    "It will return the pipeline name and store object for use by the next function.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- A unique identifier for the pipeline is passed to the function.\n",
    "- Some files appear in something that functions like a directory (s3 bucket, actual directory).\n",
    "- That directory is passed to this function.\n",
    "\n",
    "Therefore some example inputs would/could be:\n",
    "\n",
    "- `cpih-22-02-2024T01ยง:01:01` or similar (some kind of unique but useful convention.\n",
    "- `s3://my-aws-bucker/dataset-cpih/23-01-2024T01:01:00/initial` or `/my-files/localdata/dataset-cpih/23-01-2024T01:01:00/initial`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfebb3b0-f2d1-4af9-8946-223761fbfbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "from jsonschema import ValidationError\n",
    "\n",
    "from deps.store.base import BaseWritableSingleDirectoryStore\n",
    "from deps import notify\n",
    "from deps import message\n",
    "from deps.other import (\n",
    "    heuristically_create_pipeline_config,\n",
    "    schema_path_from_config,\n",
    "    directory_store_from_pathlike_source,\n",
    "    validate_json,\n",
    "    get_supplementary_distribution_patterns,\n",
    "    get_required_file_patterns\n",
    ")\n",
    "\n",
    "def confirm_configuration(pipeline_identifier: str, pathlike_source: str) -> Tuple[str, BaseWritableSingleDirectoryStore]:\n",
    "    \"\"\"\n",
    "    The purpose of this function is to: \n",
    "\n",
    "    - (a) Confirm the submisson directory contains a file \"pipeline-config.json\"\n",
    "    - (b) Create one where it does not using the files that were submitted\n",
    "    - (c) Validate the \"pipeline-config.json\"\n",
    "\n",
    "    Returns the pipeline name and the store ready for use by the next function.\n",
    "    \"\"\"\n",
    "\n",
    "    store: BaseWritableSingleDirectoryStore = directory_store_from_pathlike_source(pathlike_source)\n",
    "    \n",
    "    # if this submissions does not have a pipeline-config.json....\n",
    "    we_created_config = False\n",
    "    if not store.has_lone_file_matching(\"pipeline-config.json\"):\n",
    "        try:\n",
    "            heuristically_create_pipeline_config(store)\n",
    "            store.add_file(\"pipeline-config.json\")\n",
    "            we_created_config = True\n",
    "        except Exception as err:\n",
    "            notify.data_engineering(message.cant_create_config(pipeline, store, err))\n",
    "            raise err\n",
    "\n",
    "\n",
    "    config_dict: dict = store.get_lone_matching_json_as_dict(\"pipeline-config.json\")\n",
    "\n",
    "    try:\n",
    "        config_schema_path = schema_path_from_config(config_dict)\n",
    "    except Exception as err:\n",
    "        notify.data_engineering(message.cant_find_scheama(config_dict, err))\n",
    "        raise err\n",
    "\n",
    "    \n",
    "    # Now we have a config and the schema for that config we validate it.\n",
    "    try:\n",
    "        validate_json(config_schema_path, data_dict=config_dict, msg=\"Some helpful message\", indent=2)\n",
    "    except ValidationError as verr:\n",
    "\n",
    "        # If the pipeline schema is not valid, tell whoever provided it (can be either us or the external submitter)\n",
    "        if we_created_config:\n",
    "            notify.data_engineering(messge.invalid_heuristically_created_config(config_dict))\n",
    "        else:\n",
    "            notify.data_submitter(message.invalid_config, error)\n",
    "            notify.data_engineering(\n",
    "                message.subitter_notified(\n",
    "                    \"Submitter notified of schema error in pipeline-config.json\",\n",
    "                    config_dict,\n",
    "                    error)\n",
    "                )\n",
    "        raise verr\n",
    "    except Exception as err:\n",
    "        # We got an unexpected error while trying to validate, let the DEs know.\n",
    "        notify.data_engineering(\n",
    "            message.unexpected_error(\n",
    "                \"Error encounted when trying to validate pipeline-config.json\",\n",
    "                err)\n",
    "            )\n",
    "        raise err\n",
    "\n",
    "    # Confirm that the files required for this pipeline and included in\n",
    "    # the submitted files.\n",
    "    for required_file_pattern in get_required_file_patterns(config_dict):\n",
    "        if not store.has_lone_file_matching(required_file_pattern):\n",
    "            msg = message.expected_file_submission_missing(\n",
    "                f\"Unable to find required file matching regex {required_file_pattern} in submission\",\n",
    "                config_dict, store, pipeline_name=pipeline_name)\n",
    "            notify.data_submitter(message.invalid_config, error)\n",
    "            notify.data_engineering(\n",
    "                message.subitter_notified(\n",
    "                    \"Submitter notified of schema error in pipeline-config.json\",\n",
    "                    config_dict,\n",
    "                    error)\n",
    "                )\n",
    "    \n",
    "    # Where one or more supplementary distributions have been specified, confirm those\n",
    "    # files exist in the directory of submitted files.\n",
    "    for supplementry_distribution_pattern in get_supplementary_distribution_patterns(config_dict):\n",
    "        if not store.has_lone_file_matching(supplementry_distribution_pattern):\n",
    "            msg = message.expected_file_submission_missing(\n",
    "                f\"Unable to find required file matching regex {supplementry_distribution_pattern} in submission\",\n",
    "                config_dict, store, pipeline_name=pipeline_name)\n",
    "\n",
    "    return pipeline_identifier, store\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81dbb69-31d2-463d-8d3b-fa4185c6c923",
   "metadata": {},
   "source": [
    "# Key Concept  - Pipeline Details\n",
    "\n",
    "In all cases the only things that are subject to change in these pipelines are:\n",
    "\n",
    "- (a) the inputs and how they are sanity checked.\n",
    "- (b) the transform (if any) that's being used by the pipeline and takes these inputs.\n",
    "\n",
    "The intention here is to capture these variations via a simple dictionary `all_pipeline_details` so the pipeline code can just be reused.\n",
    "\n",
    "The keys in this dictionary (\"sdmx.default\" in the example) will be taken from the pipeline config that's in play whenever a pipeline is ran but otherwise the pipeline code itself remains static.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b417754-66da-4631-9327-05fb3bde96f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.transform.sdmx import smdx_default_v1, sdmx_sanity_check_v1\n",
    "\n",
    "all_pipeline_details = {\n",
    "\t\"sdmx.default\": {\n",
    "\t\t\"transform\": smdx_default_v1,\n",
    "\t\t\"transform_inputs\": {\n",
    "\t\t\t\"*.sdmx\": sdmx_sanity_check_v1\n",
    "\t\t},\n",
    "\t\t\"transform_kwargs\": {}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7caf3-e2ab-4fef-a56d-19020aff0786",
   "metadata": {},
   "source": [
    "The intention here is to capture the steps of a transform with a generic pattern so creating a new transform is just a matter of adding a new entry to the `all_pipeline_details` dictionary rather than recoding or creating bespoke pipelines.\n",
    "\n",
    "The above is is effectively saying:\n",
    "\n",
    "- if the \"pipeline\" specified in pipeline-config.json is `sdmx.default`\n",
    "- we should use the trasform function `sdmx_default_v1`\n",
    "- we have one input which is the file from the directory store that matches the regex `*.sdmx` (if a transform required multiple inputs, there'd be multiple key value pairs here).\n",
    "- the  inputs should be validated by the associated \"sanity_check\" function, i.e the sdmx in the example should be sanity checked  with the `sdmx_sanity_check_v1` function.\n",
    "- this transform takes no keyword arguments (future proofing, its concievable DEs might need to do this in  some scenarios to increare reusibility:`{\"fillna\": True}` or somesuch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463a3b1-e2a2-4c9c-8c77-14deed94786a",
   "metadata": {},
   "source": [
    "# Function/Step 2 - Transform\n",
    "\n",
    "The following is a generic transform step that makes use of \"pipeline details\" provided via this structure.\n",
    "\n",
    "This will create a new csv and metadata file and write to the current working directory.\n",
    "\n",
    "It will return the pipeline name and store object for use by the next function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58e9ec87-c8dc-491c-b30e-172f39ae0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "from jsonschema import ValidationError\n",
    "\n",
    "from deps.store.base import BaseWritableSingleDirectoryStore\n",
    "from deps import notify\n",
    "from deps.other import (\n",
    "    get_pipeline_identifier_from_config,\n",
    "    validate_json,\n",
    "    UploadServiceClient,\n",
    "    get_supplementary_distribution_patterns,\n",
    "    validate_csv\n",
    ")\n",
    "\n",
    "def transform(pipeline_name, store: BaseWritableSingleDirectoryStore) -> Tuple[str, BaseWritableSingleDirectoryStore]:\n",
    "\t\n",
    "    config_dict: dict = store.get_lone_matching_json_as_dict(\"pipeline-config.json\")\n",
    "    pipeline_identifier = get_pipeline_identifier_from_config(config_dict)\n",
    "    \n",
    "    # Use that to get the details we need to run this pipeline\n",
    "    pipeline_details = all_pipeline_details.get(pipeline_identifier, None)\n",
    "    \n",
    "    if pipeline_details is None:\n",
    "        msg = message.unknown_pipeline(pipeline_identifier, config_dict)\n",
    "        notify.data_engineering(msg)\n",
    "        raise ValueError(msg)\n",
    "    \n",
    "    args = []\n",
    "    for match, sanity_checker in pipeline_details[\"transform_inputs\"].items():\n",
    "        try:\n",
    "            input_file_path: Path = store.save_lone_file_matching(match)\n",
    "        except Exception as err:\n",
    "            notify.data_engineering(message.pipeline_input_exception(pipeline_details, store, err))\n",
    "            raise err\n",
    "\n",
    "        try:\n",
    "            sanity_checker(input_file_path)\n",
    "        except Exception as err:\n",
    "            notify.data_engineering(message.pipeline_input_sanity_check_exception(pipeline_details, store, err))\n",
    "            raise err\n",
    "        \n",
    "        args.append(input_file_path)\n",
    "\n",
    "    kwargs = pipeline_details[\"transform_kwargs\"]\n",
    "    transform_function = pipeline_details[\"transform\"]\n",
    "    \n",
    "    try:\n",
    "        csv_path, metadata_path = transform_function(*args, **kwargs)\n",
    "    except Exception as err:\n",
    "        # Something has gone wrong in the transform, let DE team know.\n",
    "        notify.data_engineering(message.error_in_transform(config_dict, store, err))\n",
    "        raise err\n",
    "\n",
    "    # Validate the metadata\n",
    "    metadata_schema = \"\" # We get this from somwhere, no idea where yet\n",
    "    try:\n",
    "        validate_json(metadata_schema, data_path=metadata_path, msg=\"Some helpful message\", indent=2)\n",
    "    except ValidationError as verr:\n",
    "        # THINK - if the artifacts are transient we probably need to store them so an enginner\n",
    "        # can see precisely what input caused the problem\n",
    "        notify.data_engineering(messge.metadata_validation_error(metadata_path, verr))\n",
    "        raise verr\n",
    "    except Exception as err:\n",
    "        # We got an unexpected error while trying to validate, let the DEs know.\n",
    "        # THINK - if the artifacts are transient we probably need to store them so an enginner\n",
    "        # can see precisely what input caused the problem\n",
    "        notify.data_engineering(\n",
    "            message.unexpected_error(\n",
    "                \"Error encounted when trying to validate dataset metadata.\",\n",
    "                err)\n",
    "            )\n",
    "        raise err\n",
    "\n",
    "    # Validate the csv\n",
    "    try:\n",
    "        validate_csv(csv_path, metadata_path)\n",
    "    except Exception as err:\n",
    "        notify.data_engineering(\n",
    "            message.unexpected_error(\n",
    "                \"Error encounted when trying to validate csv.\",\n",
    "                err)\n",
    "            )\n",
    "        # THINK - if the artifacts are transient we probably need to store them so an enginner\n",
    "        # can see precisely what input caused the problem\n",
    "        raise err\n",
    "        \n",
    "    return pipeline_name, store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd38a5f-533f-4230-923a-b097acacff00",
   "metadata": {},
   "source": [
    "# Function/Step 3 - Upload\n",
    "\n",
    "The intention here is to upload the metadata and data to the appropriate website backend services.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "- a csv file exists in the working directory as `data.csv`.\n",
    "- a json metadata exists in the working directoryu as `metadata.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06b983da-7472-4b1c-8b10-dc1261c93f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from pathlib import Path\n",
    "\n",
    "from deps.other import DatasetApiClient\n",
    "\n",
    "def upload(pipeline_name, store: BaseWritableSingleDirectoryStore):\n",
    "    \"\"\"\n",
    "    Upload the data files to the dp-upload-service and the metadata to the dp-dataaset-api.\n",
    "    \"\"\"\n",
    "\n",
    "    config_dict: dict = store.get_lone_matching_json_as_dict(\"pipeline-config.json\")\n",
    "\n",
    "    csv_path = Path(\"data.csv\")\n",
    "    if not csv_path.exists():\n",
    "        msg = f'The expected file \"{csv_path}\" does not exist'\n",
    "        notify.data_engineering(\n",
    "            message.expected_local_file_missing(\n",
    "                msg,\n",
    "                csv_path,\n",
    "                pipeline_name\n",
    "            )\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    metadata_path = Path(\"metadata.json\")\n",
    "    if not metadata_path.exists():\n",
    "        msg = f'The expected file \"{metadata_path}\" does not exist'\n",
    "        notify.data_engineering(\n",
    "            message.expected_local_file_missing(\n",
    "                msg,\n",
    "                csv_path,\n",
    "                pipeline_name\n",
    "            )\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "        \n",
    "    # Upload files that are just files\n",
    "    files_to_upload = [csv_path]\n",
    "\n",
    "    for supplementry_distribution in get_supplementary_distribution_patterns(config_dict):\n",
    "        # Download the supplementary file\n",
    "        file_to_upload: Path = store.save_lone_file_matching(supplementry_distribution)\n",
    "        files_to_upload.append(file_to_upload)\n",
    "\n",
    "    upload_service_client = UploadServiceClient()\n",
    "    for file_to_upload in files_to_upload:\n",
    "        try:\n",
    "            upload_service_client.upload(file_to_upload)\n",
    "        except HttpException as err:\n",
    "            notify.software_engineering(\n",
    "                message.unexpected_error(\n",
    "                    \"Error encounted when trying to upload csv data to the upload service.\",\n",
    "                    err)\n",
    "                )\n",
    "        \n",
    "    # Upload the metadata to the dp-dataset-api\n",
    "    dataset_api_client = DatasetApiClient()\n",
    "    try:\n",
    "        new_dataset_url_or_identifier = dataset_api_client.upload_metadata(metadata_path)\n",
    "    except Exception as err:\n",
    "        notify.software_engineering(\n",
    "            message.unexpected_error(\n",
    "                \"Error encounted when trying to upload metadata to the dataset api.\",\n",
    "                err)\n",
    "            )\n",
    "\n",
    "    new_dataset_url_or_identifier = \"TODO - create me somehow, probably in the metadata\"\n",
    "    \n",
    "    notify.publishing_support(f\"The dataset {new_dataset_url_or_identifier} is ready for use in the publication process\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2e5c3b-51ba-4b7f-a665-d3eca345a306",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "To use (if all the functions and class were implemented) you'd do something like the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "024ad11f-3fe8-4a10-b487-3fc074dfca59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deps.store.s3fake import FakeS3DirectoryStore\n",
    "\n",
    "pipeline_name, store = confirm_configuration(\n",
    "        \"cpih-22-01-2024\",\n",
    "        \"s3://some-aws-bucket/dataset-cpih/22-01-2024T00:00:00/initial\")\n",
    "\n",
    "pipeline_name, store = transform(pipeline_name, store)\n",
    "\n",
    "upload(pipeline_name,\n",
    "       store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
